{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ochekroun/projet-ia/blob/master/classification-images/Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15cc-9-f-5AQ"
      },
      "outputs": [],
      "source": [
        "# !python -V"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CR0SNc0n9Ytk"
      },
      "outputs": [],
      "source": [
        "#!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-Lt0QEL0E-5b",
        "outputId": "89066493-ac43-4206-c5b3-634c3e04f393"
      },
      "outputs": [],
      "source": [
        "# !pip install tensorflow==2.16.2\n",
        "# !pip install keras==3.4.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fV3J_EblNKDC"
      },
      "outputs": [],
      "source": [
        "# utilisons pytorch plutôt que tensorflow\n",
        "import os\n",
        "os.environ[\"KERAS_BACKEND\"] = \"torch\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_nFxcT388Pk"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras import layers\n",
        "from keras.applications import EfficientNetB0\n",
        "import requests\n",
        "import tarfile\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import learning_curve\n",
        "import json\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPaAM2tDVPGG"
      },
      "outputs": [],
      "source": [
        "#!rm -rf ./data/nuswide_81/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKO1owWH88Pm",
        "outputId": "aa4c67b2-346f-4b94-f11b-7fd654f9d306"
      },
      "outputs": [],
      "source": [
        "download_file = False\n",
        "\n",
        "from pathlib import Path\n",
        "Path(\"./data\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "if download_file:\n",
        "    def decompress(tar_file, file_path, members=None):\n",
        "        \"\"\"\n",
        "        Extracts `tar_file` and puts the `members` to `path`.\n",
        "        If members is None, all members on `tar_file` will be extracted.\n",
        "        \"\"\"\n",
        "        tar = tarfile.open(tar_file, mode=\"r:gz\")\n",
        "        if members is None:\n",
        "            members = tar.getmembers()\n",
        "        # with progress bar\n",
        "        # set the progress bar\n",
        "        progress = tqdm(members)\n",
        "        for member in progress:\n",
        "            tar.extract(member, path=file_path)\n",
        "            # set the progress description of the progress bar\n",
        "            progress.set_description(f\"Extracting {member.name}\")\n",
        "\n",
        "        tar.close()\n",
        "\n",
        "    def download(url, file_path):\n",
        "        # Streaming, so we can iterate over the response.\n",
        "        response = requests.get(url, stream=True)\n",
        "\n",
        "        # Sizes in bytes.\n",
        "        total_size = int(response.headers.get(\"content-length\", 0))\n",
        "        block_size = 4096\n",
        "\n",
        "        with tqdm(total=total_size, unit=\"B\", unit_scale=True) as progress_bar:\n",
        "            with open(file_path, \"wb\") as file:\n",
        "                for data in response.iter_content(block_size):\n",
        "                    progress_bar.update(len(data))\n",
        "                    file.write(data)\n",
        "\n",
        "        if total_size != 0 and progress_bar.n != total_size:\n",
        "            raise RuntimeError(\"Could not download file\")\n",
        "\n",
        "\n",
        "    download('https://olivierchekroun.blob.core.windows.net/documents/projet-ia/nus_wide.tar.gz','./data/nus_wide.tar.gz')\n",
        "    decompress('./data/nus_wide.tar.gz', './data/nuswide_81')\n",
        "    \n",
        "    download('https://olivierchekroun.blob.core.windows.net/documents/projet-ia/data.tar.gz','./data/data.tar.gz')\n",
        "    decompress('./data/data.tar.gz', './data/')\n",
        "\n",
        "\n",
        "\n",
        "check_image_file = False\n",
        "if check_image_file:\n",
        "    image_dir = './data/images'\n",
        "    bad_image_file_path = './data/bad_images.txt'\n",
        "    IMG_SIZE = 224\n",
        "    size = (IMG_SIZE, IMG_SIZE)\n",
        "    count = 0\n",
        "    try:\n",
        "        os.remove(bad_image_file_path)\n",
        "    except OSError:\n",
        "        pass\n",
        "    arr = os.listdir(image_dir)\n",
        "    print(len(arr))\n",
        "    bad_files = []\n",
        "    for image_name in tqdm(arr):\n",
        "        count += 1\n",
        "        imagePath = os.path.join(image_dir, image_name)\n",
        "        try:\n",
        "            image = Image.open(imagePath)\n",
        "            image.load()\n",
        "            image = tf.image.resize(image, size)\n",
        "            image = image.numpy()\n",
        "        except Exception as e:\n",
        "            print(f'{e} {imagePath}')\n",
        "            bad_files.append(image_name)\n",
        "            continue\n",
        "    print(f'Images count: {count}')\n",
        "\n",
        "    print(bad_files)\n",
        "\n",
        "    with open(bad_image_file_path, 'w') as f:\n",
        "        for line in bad_files:\n",
        "            f.write(f\"{line}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9sdRxe2z88Pm"
      },
      "outputs": [],
      "source": [
        "def load_labels():\n",
        "    labels = []\n",
        "    with open('./data/labels.txt') as f:\n",
        "        lines = f.read().splitlines()\n",
        "        for line in lines:\n",
        "            labels.append(line)\n",
        "    return labels\n",
        "\n",
        "def load_dataframe(file_name):\n",
        "    with open('./data/bad_images.txt', 'r') as file:\n",
        "        bad_files = file.read().splitlines()\n",
        "\n",
        "    print(f'Bad files {bad_files}')\n",
        "\n",
        "    labels = load_labels()\n",
        "\n",
        "    # Étape 1 : Charger la liste des images\n",
        "\n",
        "    with open(file_name) as f:\n",
        "        all_samples_data = f.read().splitlines()\n",
        "\n",
        "\n",
        "    image_names =[]\n",
        "    string_values = []\n",
        "    array_values = []\n",
        "    given_labels = []\n",
        "    for data_line in tqdm(all_samples_data):\n",
        "        parts = data_line.split(' ')\n",
        "        image_name = parts[0]\n",
        "        array_value = np.asarray(list(map(int, parts[1:])))\n",
        "        string_value = ' '.join(parts[1:])\n",
        "\n",
        "        given_label = np.array(labels)[np.argwhere(array_value)[:, 0]]\n",
        "        #print(given_labels)\n",
        "\n",
        "        image_names.append(image_name)\n",
        "        string_values.append(string_value)\n",
        "        array_values.append(array_value)\n",
        "        given_labels.append(given_label)\n",
        "\n",
        "    # Étape 3 : Créer le DataFrame\n",
        "    data = {'Image': image_names, 'Values': array_values, 'String_Values': string_values, 'Given_Labels': given_labels}\n",
        "    df = pd.DataFrame(data)\n",
        "    print(f'Dataframe length: {len(df)}')\n",
        "\n",
        "    # Étape 4 : Exclure les lignes où il n'y a que des 0 dans les tableaux de valeurs\n",
        "    df_filtered = df\n",
        "    df_filtered = df[df['Values'].apply(lambda x: any(x))]\n",
        "    df_filtered = df_filtered[~df_filtered['Image'].isin(bad_files)]\n",
        "    # if stratify :\n",
        "    #     df_filtered = df_filtered.groupby('String_Values').filter(lambda x: len(x) > 1)\n",
        "    print(f'Dataframe filtered length: {len(df_filtered)}')\n",
        "\n",
        "    return df_filtered"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yeasR2LM88Pn",
        "outputId": "e9fb9243-0ccc-4900-853f-fe8a82012e14"
      },
      "outputs": [],
      "source": [
        "df = load_dataframe('./data/database.txt')\n",
        "labels = load_labels()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YnEqVKv888Pn",
        "outputId": "b44ac805-f933-4b48-f3a8-5fb72a8d63b7"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "stratify = False\n",
        "num_class = df['String_Values'].nunique()\n",
        "print(num_class)\n",
        "train_size = 1000\n",
        "val_size = num_class if stratify else int(train_size * 0.2)\n",
        "test_size = 0.5\n",
        "\n",
        "if stratify:\n",
        "    train_df, val_df = train_test_split(\n",
        "        df,\n",
        "        stratify = df['String_Values'].values,\n",
        "        train_size = train_size,\n",
        "        test_size= val_size\n",
        "        )\n",
        "else:\n",
        "    train_df, val_df = train_test_split(\n",
        "        df,\n",
        "        train_size = train_size,\n",
        "        test_size= val_size\n",
        "        )\n",
        "\n",
        "    train_df, test_df = train_test_split(\n",
        "        train_df,\n",
        "        test_size = test_size\n",
        "    )\n",
        "\n",
        "print(\"Number of images for training: \", len(train_df))\n",
        "print(\"Number of images for validation: \", len(val_df))\n",
        "print(\"Number of images for test: \", len(test_df))\n",
        "\n",
        "\n",
        "\n",
        "# df = load_dataframe('./data/nuswide_81/test.txt')\n",
        "# num_class = df['String_Values'].nunique()\n",
        "# print(num_class)\n",
        "# val_size = num_class if stratify else int(train_size * 0.2)\n",
        "# if stratify:\n",
        "#     test_df, _ = train_test_split(\n",
        "#         df,\n",
        "#         stratify = df['String_Values'].values,\n",
        "#         train_size=val_size\n",
        "#         )\n",
        "# else:\n",
        "#     test_df, _ = train_test_split(\n",
        "#     df,\n",
        "#     train_size=val_size,\n",
        "# )\n",
        "# print(\"Number of images for test: \", len(test_df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GShNzsCC88Po"
      },
      "outputs": [],
      "source": [
        "def write_df(df, labels, file_path):\n",
        "    rows = []\n",
        "    for index, row in test_df[['Image', 'Given_Labels']].iterrows():\n",
        "        row_image = row['Image']\n",
        "        row_labels = row['Given_Labels'].tolist()\n",
        "        row = {'image_name': row_image, 'image_labels': row_labels}\n",
        "        rows.append(row)\n",
        "    with open(file_path, 'w') as fp:\n",
        "        json.dump({'samples': rows, 'labels': labels}, fp, indent=3)\n",
        "\n",
        "write_df(train_df, labels, './data/train.json')\n",
        "write_df(val_df, labels, './data/val.json')\n",
        "write_df(test_df, labels, './data/test.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ZboKjdE88Po"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "class NusDataset(Dataset):\n",
        "    def __init__(self, anno_path, transforms):\n",
        "        self.transforms = transforms\n",
        "        with open(anno_path) as fp:\n",
        "            json_data = json.load(fp)\n",
        "        samples = json_data['samples']\n",
        "        self.classes = json_data['labels']\n",
        "\n",
        "        self.imgs = []\n",
        "        self.annos = []\n",
        "        print('loading', anno_path)\n",
        "        for sample in samples:\n",
        "            self.imgs.append(sample['image_name'])\n",
        "            self.annos.append(sample['image_labels'])\n",
        "        for item_id in range(len(self.annos)):\n",
        "            item = self.annos[item_id]\n",
        "            vector = [cls in item for cls in self.classes]\n",
        "            self.annos[item_id] = np.array(vector, dtype=float)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        anno = self.annos[item]\n",
        "        try:\n",
        "            IMG_SIZE = 224\n",
        "            size = (IMG_SIZE, IMG_SIZE)\n",
        "            img_path = self.imgs[item]\n",
        "            image = Image.open(img_path)\n",
        "            if self.transforms is not None:\n",
        "                image = self.transforms(image)\n",
        "            #image = tf.image.resize(image, size)\n",
        "            # image = image.numpy()\n",
        "        except Exception as e:\n",
        "            print(f'{e} {img_path}')\n",
        "        return image, anno\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n",
        "\n",
        "def image_processing(image):\n",
        "    IMG_SIZE = 224\n",
        "    size = (IMG_SIZE, IMG_SIZE)    \n",
        "    image = tf.image.resize(image, size)\n",
        "    image = image.numpy()\n",
        "    return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhYm2YqdU0D-",
        "outputId": "9ad0ab0e-22b6-4c9b-9652-9b8fa63f9e3f"
      },
      "outputs": [],
      "source": [
        "#!ls ./data/nuswide_81/images/11781_1590858672_d446580056_m.jpg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XWjIfiaY88Po",
        "outputId": "77a79062-b9e0-4397-d7bf-9de5044ff0e1"
      },
      "outputs": [],
      "source": [
        "# Let's take a look at the data we have. To do it we need to load the dataset without augmentations.\n",
        "samples_ds = NusDataset('./data/test.json', None)\n",
        "\n",
        "# A simple function for visualization.\n",
        "def show_sample(img, binary_img_labels):\n",
        "    # Convert the binary labels back to the text representation.\n",
        "    img_labels = np.array(samples_ds.classes)[np.argwhere(binary_img_labels > 0)[:, 0]]\n",
        "    plt.imshow(img)\n",
        "    plt.title(\"{}\".format(', '.join(img_labels)))\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "for sample_id in range(5):\n",
        "    img, labels = samples_ds[sample_id]\n",
        "    show_sample(img, labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jz9pwYxf88Po",
        "outputId": "978aaad1-ba79-457c-fb77-ebd4a4a4587b"
      },
      "outputs": [],
      "source": [
        "train_ds = NusDataset('./data/train.json', image_processing)\n",
        "val_ds = NusDataset('./data/val.json', image_processing)\n",
        "test_ds = NusDataset('./data/test.json', image_processing)\n",
        "\n",
        "batch_size = 32\n",
        "num_workers = 8\n",
        "num_classes = len(train_ds.classes)\n",
        "\n",
        "train_dl = DataLoader(train_ds, batch_size=batch_size, num_workers=num_workers, shuffle=True, drop_last=True)\n",
        "val_dl = DataLoader(val_ds, batch_size=batch_size, num_workers=num_workers)\n",
        "test_dl = DataLoader(test_ds, batch_size=batch_size, num_workers=num_workers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l9cp0DjG88Pp"
      },
      "outputs": [],
      "source": [
        "def build_model(num_classes):\n",
        "    inputs = layers.Input(shape=(224, 224, 3))\n",
        "\n",
        "    # On retire les dernières couches avec \"include_top=False\"\n",
        "    model = EfficientNetB0(include_top=False, input_tensor=inputs, weights=\"imagenet\")\n",
        "\n",
        "    # \"Glace\" the paramètres pré-entrainés\n",
        "    model.trainable = False\n",
        "\n",
        "    # On rajoute les couches retirées - ces couches sont entrainables !\n",
        "    x = layers.GlobalAveragePooling2D(name=\"avg_pool\")(model.output)\n",
        "\n",
        "    # On peut ajuster le ratio de neurones désactivés pour la couche Dropout\n",
        "    top_dropout_rate = 0.2\n",
        "    x = layers.Dropout(top_dropout_rate, name=\"top_dropout\")(x)\n",
        "    outputs = layers.Dense(num_classes, activation='sigmoid', name=\"predictions\")(x)\n",
        "\n",
        "    # Compile le nouveau modèle\n",
        "    model = keras.Model(inputs, outputs, name=\"EfficientNet\")\n",
        "\n",
        "    # On augmente le learning rate du défaut 0.001 à 0.01\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=1e-2)\n",
        "\n",
        "    model.compile(\n",
        "         optimizer=optimizer,\n",
        "         loss=tf.keras.metrics.binary_crossentropy,\n",
        "         metrics=[\"binary_accuracy\"]\n",
        "    )\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4hVOSA_88Pp"
      },
      "outputs": [],
      "source": [
        "model = build_model(num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePm3Zevd88Pq"
      },
      "outputs": [],
      "source": [
        "def unfreeze_model(model):\n",
        "    # On rends entrainable les derniers 20 layers (sauf ceux de BatchNormalization)\n",
        "    for layer in model.layers[-20:]:\n",
        "        if not isinstance(layer, layers.BatchNormalization):\n",
        "          layer.trainable = True\n",
        "\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=1e-5)\n",
        "    model.compile(\n",
        "        optimizer=optimizer, loss=tf.keras.metrics.binary_crossentropy, metrics=[\"binary_accuracy\"]\n",
        "    )\n",
        "\n",
        "\n",
        "unfreeze_model(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qf_KC63xEVGu",
        "outputId": "0f751f5d-0307-4c67-fd45-c00a7d9f1c54"
      },
      "outputs": [],
      "source": [
        "#!pip list| grep keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPRNBK8Y88Pq",
        "outputId": "0464629f-eaa5-4d1c-b2c6-dd61d1749cff"
      },
      "outputs": [],
      "source": [
        "history = model.fit(x=train_dl,validation_data=val_dl,epochs=10)\n",
        "model.save('simple_model_without_unfreeze.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Retrieve data from the history\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "binary_accuracy = history.history['binary_accuracy']\n",
        "val_binary_accuracy = history.history['val_binary_accuracy']\n",
        "\n",
        "# Plot the loss curves\n",
        "plt.figure(figsize=(14, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(loss, label='Training Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.title('Loss Curve')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Plot the accuracy curves\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(binary_accuracy, label='Training Accuracy')\n",
        "plt.plot(val_binary_accuracy, label='Validation Accuracy')\n",
        "plt.title('Accuracy Curve')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRQafyn588Pq"
      },
      "outputs": [],
      "source": [
        "model = keras.saving.load_model('simple_model.keras')\n",
        "model.evaluate(x=test_dl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "l8Vo36lH88Pq",
        "outputId": "332ecc2a-db9f-45b2-986a-0606481a5dac"
      },
      "outputs": [],
      "source": [
        "# Run inference on the test data\n",
        "samples_ds = NusDataset('./data/test.json', image_processing)\n",
        "for sample_id in range(200, 250):\n",
        "    test_img, test_labels = samples_ds[sample_id]\n",
        "    test_img_path = samples_ds.imgs[sample_id]\n",
        "    test_img = np.expand_dims(test_img, axis=0)\n",
        "    # raw_pred  = model(test_img, training = False)[0]\n",
        "    raw_pred = model(test_img, training = False).cpu()[0]\n",
        "    raw_pred = np.array(raw_pred > 0.5, dtype=float)\n",
        "\n",
        "    predicted_labels = np.array(samples_ds.classes)[np.argwhere(raw_pred > 0)[:, 0]]\n",
        "    print(len(predicted_labels))\n",
        "    if not len(predicted_labels):\n",
        "        predicted_labels = ['no predictions']\n",
        "    img_labels = np.array(samples_ds.classes)[np.argwhere(test_labels > 0)[:, 0]]\n",
        "    plt.imshow(Image.open(test_img_path))\n",
        "    plt.title(\"Predicted labels: {} \\nGT labels: {}\".format(', '.join(predicted_labels), ', '.join(img_labels)))\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "directory_path = './data/images_persos/xxx/'\n",
        "\n",
        "# List all files and directories\n",
        "for entry in os.listdir(directory_path):\n",
        "    full_path = os.path.join(directory_path, entry)\n",
        "    if os.path.isfile(full_path):\n",
        "        print(entry)\n",
        "\n",
        "        image = Image.open(full_path)\n",
        "        test_img = image_processing(image)\n",
        "        test_img = np.expand_dims(test_img, axis=0)\n",
        "        raw_pred = model(test_img, training = False).cpu()[0]\n",
        "        raw_pred = np.array(raw_pred > 0.5, dtype=float)\n",
        "\n",
        "        predicted_labels = np.array(samples_ds.classes)[np.argwhere(raw_pred > 0)[:, 0]]\n",
        "        print(len(predicted_labels))\n",
        "        if not len(predicted_labels):\n",
        "            predicted_labels = ['no predictions']        \n",
        "\n",
        "        plt.title(\"Predicted labels: {}\".format(', '.join(predicted_labels)))\n",
        "        \n",
        "        plt.imshow(image)\n",
        "        plt.axis('off')\n",
        "        plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
