{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "from keras.applications import EfficientNetB0\n",
    "import requests\n",
    "import tarfile \n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_file = False\n",
    "if download_file:\n",
    "    url = 'https://olivierchekroun.blob.core.windows.net/documents/projet-ia/nus_wide.tar.gz'\n",
    "    file_path = './data/nus_wide.tar.gz'\n",
    "    print(\"Download file...\")\n",
    "    response = requests.get(url, stream=True)\n",
    "    if response.status_code == 200:\n",
    "        with open(file_path, 'wb') as f:\n",
    "            f.write(response.raw.read())    \n",
    "    \n",
    "    print(\"Extract images...\")\n",
    "    file = tarfile.open(file_path) \n",
    "    file.extractall('./data/nuswide_81') \n",
    "    file.close() \n",
    "\n",
    "check_image_file = False\n",
    "if check_image_file:\n",
    "    image_dir = './data/images'\n",
    "    bad_image_file_path = './data/bad_images.txt'\n",
    "    IMG_SIZE = 224\n",
    "    size = (IMG_SIZE, IMG_SIZE)    \n",
    "    count = 0\n",
    "    try:\n",
    "        os.remove(bad_image_file_path)\n",
    "    except OSError:\n",
    "        pass\n",
    "    arr = os.listdir(image_dir)\n",
    "    print(len(arr))\n",
    "    bad_files = []\n",
    "    for image_name in tqdm(arr):\n",
    "        count += 1\n",
    "        imagePath = os.path.join(image_dir, image_name)\n",
    "        try:            \n",
    "            image = Image.open(imagePath)\n",
    "            image.load()\n",
    "            image = tf.image.resize(image, size)\n",
    "            image = image.numpy()\n",
    "        except Exception as e:\n",
    "            print(f'{e} {imagePath}')\n",
    "            bad_files.append(image_name)\n",
    "            continue    \n",
    "    print(f'Images count: {count}')\n",
    "\n",
    "    print(bad_files)\n",
    "\n",
    "    with open(bad_image_file_path, 'w') as f:\n",
    "        for line in bad_files:\n",
    "            f.write(f\"{line}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_labels():\n",
    "    labels = []\n",
    "    with open('./data/labels.txt') as f:\n",
    "        lines = f.read().splitlines()\n",
    "        for line in lines:\n",
    "            labels.append(line)\n",
    "    return labels\n",
    "\n",
    "def load_dataframe(file_name):\n",
    "    with open('./data/bad_images.txt', 'r') as file:\n",
    "        bad_files = file.read().splitlines()\n",
    "\n",
    "    print(f'Bad files {bad_files}')\n",
    "\n",
    "    labels = load_labels()\n",
    "\n",
    "    # Étape 1 : Charger la liste des images\n",
    "\n",
    "    with open(file_name) as f:\n",
    "        all_samples_data = f.read().splitlines()\n",
    "\n",
    "\n",
    "    image_names =[]\n",
    "    string_values = []\n",
    "    array_values = []\n",
    "    given_labels = []\n",
    "    for data_line in tqdm(all_samples_data):\n",
    "        parts = data_line.split(' ')\n",
    "        image_name = parts[0]\n",
    "        array_value = np.asarray(list(map(int, parts[1:])))\n",
    "        string_value = ' '.join(parts[1:])\n",
    "\n",
    "        given_label = np.array(labels)[np.argwhere(array_value)[:, 0]]\n",
    "        #print(given_labels)\n",
    "\n",
    "        image_names.append(image_name)\n",
    "        string_values.append(string_value)\n",
    "        array_values.append(array_value)\n",
    "        given_labels.append(given_label)\n",
    "\n",
    "    # Étape 3 : Créer le DataFrame\n",
    "    data = {'Image': image_names, 'Values': array_values, 'String_Values': string_values, 'Given_Labels': given_labels}\n",
    "    df = pd.DataFrame(data)\n",
    "    print(f'Dataframe length: {len(df)}')\n",
    "\n",
    "    # Étape 4 : Exclure les lignes où il n'y a que des 0 dans les tableaux de valeurs\n",
    "    df_filtered = df\n",
    "    df_filtered = df[df['Values'].apply(lambda x: any(x))]\n",
    "    df_filtered = df_filtered[~df_filtered['Image'].isin(bad_files)]\n",
    "    # if stratify :\n",
    "    #     df_filtered = df_filtered.groupby('String_Values').filter(lambda x: len(x) > 1)\n",
    "    print(f'Dataframe filtered length: {len(df_filtered)}')\n",
    "\n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_dataframe('./data/database.txt')\n",
    "labels = load_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "stratify = False\n",
    "num_class = df['String_Values'].nunique()\n",
    "print(num_class)\n",
    "train_size = 5000\n",
    "val_size = num_class if stratify else int(train_size * 0.2)\n",
    "test_size = 0.5\n",
    "\n",
    "if stratify:\n",
    "    train_df, val_df = train_test_split(\n",
    "        df,\n",
    "        stratify = df['String_Values'].values,\n",
    "        train_size = train_size,\n",
    "        test_size= val_size\n",
    "        )\n",
    "else:\n",
    "    train_df, val_df = train_test_split(\n",
    "        df,\n",
    "        train_size = train_size,\n",
    "        test_size= val_size\n",
    "        )\n",
    "    \n",
    "    train_df, test_df = train_test_split(\n",
    "        train_df,\n",
    "        test_size = test_size\n",
    "    )\n",
    "\n",
    "print(\"Number of images for training: \", len(train_df))\n",
    "print(\"Number of images for validation: \", len(val_df))\n",
    "print(\"Number of images for test: \", len(test_df))\n",
    "\n",
    "\n",
    "\n",
    "# df = load_dataframe('./data/nuswide_81/test.txt')\n",
    "# num_class = df['String_Values'].nunique()\n",
    "# print(num_class)\n",
    "# val_size = num_class if stratify else int(train_size * 0.2)\n",
    "# if stratify:\n",
    "#     test_df, _ = train_test_split(\n",
    "#         df,\n",
    "#         stratify = df['String_Values'].values,\n",
    "#         train_size=val_size\n",
    "#         )\n",
    "# else:\n",
    "#     test_df, _ = train_test_split(\n",
    "#     df,    \n",
    "#     train_size=val_size,\n",
    "# )\n",
    "# print(\"Number of images for test: \", len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_df(df, labels, file_path):\n",
    "    rows = []\n",
    "    for index, row in test_df[['Image', 'Given_Labels']].iterrows():\n",
    "        row_image = row['Image']\n",
    "        row_labels = row['Given_Labels'].tolist()\n",
    "        row = {'image_name': row_image, 'image_labels': row_labels}\n",
    "        rows.append(row)\n",
    "    with open(file_path, 'w') as fp:\n",
    "        json.dump({'samples': rows, 'labels': labels}, fp, indent=3)\n",
    "\n",
    "write_df(train_df, labels, './data/train.json')\n",
    "write_df(val_df, labels, './data/val.json')\n",
    "write_df(test_df, labels, './data/test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NusDataset(Dataset):\n",
    "    def __init__(self, anno_path, transforms = None):\n",
    "        self.transforms = transforms\n",
    "        with open(anno_path) as fp:\n",
    "            json_data = json.load(fp)\n",
    "        samples = json_data['samples']\n",
    "        self.classes = json_data['labels']\n",
    "\n",
    "        self.imgs = []\n",
    "        self.annos = []\n",
    "        print('loading', anno_path)\n",
    "        for sample in samples:\n",
    "            self.imgs.append(sample['image_name'])\n",
    "            self.annos.append(sample['image_labels'])\n",
    "        for item_id in range(len(self.annos)):\n",
    "            item = self.annos[item_id]\n",
    "            vector = [cls in item for cls in self.classes]\n",
    "            self.annos[item_id] = np.array(vector, dtype=float)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        anno = self.annos[item]\n",
    "        try:            \n",
    "            IMG_SIZE = 224\n",
    "            size = (IMG_SIZE, IMG_SIZE)\n",
    "            img_path = self.imgs[item]\n",
    "            image = Image.open(img_path)\n",
    "            if self.transforms is not None:\n",
    "                image = self.transforms(image)            \n",
    "            image = tf.image.resize(image, size)\n",
    "            image = image.numpy()\n",
    "        except Exception as e:\n",
    "            print(f'{e} {img_path}')            \n",
    "        return image, anno\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the data we have. To do it we need to load the dataset without augmentations.\n",
    "dataset_val = NusDataset('./data/test.json', None)\n",
    "\n",
    "# A simple function for visualization.\n",
    "def show_sample(img, binary_img_labels):\n",
    "    # Convert the binary labels back to the text representation.    \n",
    "    img_labels = np.array(dataset_val.classes)[np.argwhere(binary_img_labels > 0)[:, 0]]\n",
    "    plt.imshow(img)\n",
    "    plt.title(\"{}\".format(', '.join(img_labels)))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "for sample_id in range(5):\n",
    "    show_sample(*dataset_val[sample_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = NusDataset('./data/train.json')\n",
    "val_ds = NusDataset('./data/val.json')\n",
    "test_ds = NusDataset('./data/test.json')\n",
    "\n",
    "\n",
    "train_dl = DataLoader(train_ds)\n",
    "val_dl = DataLoader(val_ds)\n",
    "test_dl = DataLoader(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(num_classes):\n",
    "    inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "\n",
    "    # On retire les dernières couches avec \"include_top=False\"\n",
    "    model = EfficientNetB0(include_top=False, input_tensor=inputs, weights=\"imagenet\")\n",
    "\n",
    "    # \"Glace\" the paramètres pré-entrainés\n",
    "    model.trainable = False\n",
    "\n",
    "    # On rajoute les couches retirées - ces couches sont entrainables !\n",
    "    x = layers.GlobalAveragePooling2D(name=\"avg_pool\")(model.output)\n",
    "\n",
    "    # On peut ajuster le ratio de neurones désactivés pour la couche Dropout\n",
    "    top_dropout_rate = 0.2\n",
    "    x = layers.Dropout(top_dropout_rate, name=\"top_dropout\")(x)\n",
    "    outputs = layers.Dense(num_classes, activation='sigmoid', name=\"predictions\")(x)\n",
    "\n",
    "    # Compile le nouveau modèle\n",
    "    model = keras.Model(inputs, outputs, name=\"EfficientNet\")\n",
    "\n",
    "    # On augmente le learning rate du défaut 0.001 à 0.01\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=1e-2)\n",
    "\n",
    "    model.compile(\n",
    "         optimizer=optimizer, \n",
    "         loss=tf.keras.metrics.binary_crossentropy,\n",
    "         metrics=[\"binary_accuracy\"]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 81\n",
    "IMG_SIZE = 224\n",
    "model = build_model(NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unfreeze_model(model):\n",
    "    # On rends entrainable les derniers 20 layers (sauf ceux de BatchNormalization)\n",
    "    for layer in model.layers[-20:]:\n",
    "        if not isinstance(layer, layers.BatchNormalization):\n",
    "          layer.trainable = True\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=1e-5)\n",
    "    model.compile(\n",
    "        optimizer=optimizer, loss=tf.keras.metrics.binary_crossentropy, metrics=[\"binary_accuracy\"]\n",
    "    )\n",
    "\n",
    "\n",
    "unfreeze_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x=train_dl,validation_data=val_dl,epochs=1)\n",
    "model.save('simple_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.saving.load_model('simple_model.keras')\n",
    "model.evaluate(x=test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference on the test data\n",
    "for sample_id in range(50):\n",
    "    test_img, test_labels = test_ds[sample_id]\n",
    "    test_img_path = test_ds.imgs[sample_id]\n",
    "    test_img = np.expand_dims(test_img, axis=0)\n",
    "    raw_pred  = model(test_img, training = False)[0]\n",
    "    raw_pred = np.array(raw_pred > 0.5, dtype=float)\n",
    "\n",
    "    predicted_labels = np.array(test_ds.classes)[np.argwhere(raw_pred > 0)[:, 0]]\n",
    "    print(len(predicted_labels))\n",
    "    if not len(predicted_labels):\n",
    "        predicted_labels = ['no predictions']\n",
    "    img_labels = np.array(test_ds.classes)[np.argwhere(test_labels > 0)[:, 0]]\n",
    "    plt.imshow(Image.open(test_img_path))\n",
    "    plt.title(\"Predicted labels: {} \\nGT labels: {}\".format(', '.join(predicted_labels), ', '.join(img_labels)))\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projet-ia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
