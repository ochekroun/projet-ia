{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ochekroun/projet-ia/blob/master/classification-images/Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_nFxcT388Pk"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras import layers\n",
        "from keras.applications import EfficientNetB0\n",
        "import requests\n",
        "import tarfile\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import learning_curve\n",
        "import json\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Téléchargements : \n",
        "- images\n",
        "- annotations\n",
        "- labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKO1owWH88Pm",
        "outputId": "aa4c67b2-346f-4b94-f11b-7fd654f9d306"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# Wall time: 20min 7s Google Colab\n",
        "download_file = False\n",
        "\n",
        "from pathlib import Path\n",
        "Path(\"./data\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "if download_file:\n",
        "    def decompress(tar_file, file_path, members=None):\n",
        "        \"\"\"\n",
        "        Extracts `tar_file` and puts the `members` to `path`.\n",
        "        If members is None, all members on `tar_file` will be extracted.\n",
        "        \"\"\"\n",
        "        tar = tarfile.open(tar_file, mode=\"r:gz\")\n",
        "        if members is None:\n",
        "            members = tar.getmembers()\n",
        "        # with progress bar\n",
        "        # set the progress bar\n",
        "        progress = tqdm(members)\n",
        "        for member in progress:\n",
        "            tar.extract(member, path=file_path)\n",
        "            # set the progress description of the progress bar\n",
        "            progress.set_description(f\"Extracting {member.name}\")\n",
        "\n",
        "        tar.close()\n",
        "\n",
        "    def download(url, file_path):\n",
        "        # Streaming, so we can iterate over the response.\n",
        "        response = requests.get(url, stream=True)\n",
        "\n",
        "        # Sizes in bytes.\n",
        "        total_size = int(response.headers.get(\"content-length\", 0))\n",
        "        block_size = 4096\n",
        "\n",
        "        with tqdm(total=total_size, unit=\"B\", unit_scale=True) as progress_bar:\n",
        "            with open(file_path, \"wb\") as file:\n",
        "                for data in response.iter_content(block_size):\n",
        "                    progress_bar.update(len(data))\n",
        "                    file.write(data)\n",
        "\n",
        "        if total_size != 0 and progress_bar.n != total_size:\n",
        "            raise RuntimeError(\"Could not download file\")\n",
        "\n",
        "\n",
        "    download('https://olivierchekroun.blob.core.windows.net/documents/projet-ia/nus_wide.tar.gz','./data/nus_wide.tar.gz')\n",
        "    decompress('./data/nus_wide.tar.gz', './data/nuswide_81')\n",
        "    \n",
        "    download('https://olivierchekroun.blob.core.windows.net/documents/projet-ia/data.tar.gz','./data/data.tar.gz')\n",
        "    decompress('./data/data.tar.gz', './data/')\n",
        "\n",
        "\n",
        "\n",
        "check_image_file = False\n",
        "if check_image_file:\n",
        "    image_dir = './data/images'\n",
        "    bad_image_file_path = './data/bad_images.txt'\n",
        "    IMG_SIZE = 224\n",
        "    size = (IMG_SIZE, IMG_SIZE)\n",
        "    count = 0\n",
        "    try:\n",
        "        os.remove(bad_image_file_path)\n",
        "    except OSError:\n",
        "        pass\n",
        "    arr = os.listdir(image_dir)\n",
        "    print(len(arr))\n",
        "    bad_files = []\n",
        "    for image_name in tqdm(arr):\n",
        "        count += 1\n",
        "        imagePath = os.path.join(image_dir, image_name)\n",
        "        try:\n",
        "            image = Image.open(imagePath)\n",
        "            image.load()\n",
        "            image = tf.image.resize(image, size)\n",
        "            image = image.numpy()\n",
        "        except Exception as e:\n",
        "            print(f'{e} {imagePath}')\n",
        "            bad_files.append(image_name)\n",
        "            continue\n",
        "    print(f'Images count: {count}')\n",
        "\n",
        "    print(bad_files)\n",
        "\n",
        "    with open(bad_image_file_path, 'w') as f:\n",
        "        for line in bad_files:\n",
        "            f.write(f\"{line}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Chargement des données brutes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9sdRxe2z88Pm"
      },
      "outputs": [],
      "source": [
        "def load_labels():\n",
        "    labels = []\n",
        "    with open('./data/labels.txt') as f:\n",
        "        lines = f.read().splitlines()\n",
        "        for line in lines:\n",
        "            labels.append(line)\n",
        "    return labels\n",
        "\n",
        "def load_dataframe(file_name):\n",
        "    with open('./data/bad_images.txt', 'r') as file:\n",
        "        bad_files = file.read().splitlines()\n",
        "\n",
        "    print(f'Bad files {bad_files}')\n",
        "\n",
        "    labels = load_labels()\n",
        "\n",
        "    # Étape 1 : Charger la liste des images\n",
        "    with open(file_name) as f:\n",
        "        all_samples_data = f.read().splitlines()\n",
        "\n",
        "    image_names =[]\n",
        "    given_labels = []\n",
        "    for data_line in tqdm(all_samples_data):\n",
        "        parts = data_line.split(' ')\n",
        "        image_name = parts[0]\n",
        "        array_value = np.asarray(list(map(int, parts[1:])))\n",
        "        given_label = np.array(labels)[np.argwhere(array_value)[:, 0]]\n",
        "        image_names.append(image_name)\n",
        "        given_labels.append(given_label)\n",
        "\n",
        "    # Étape 3 : Créer le DataFrame\n",
        "    data = {'image': image_names, 'labels': given_labels}\n",
        "    df = pd.DataFrame(data)\n",
        "    print(f'Dataframe length: {len(df)}')\n",
        "\n",
        "    # Étape 4 : Exclure les lignes où il n'y a que des 0 dans les tableaux de valeurs\n",
        "    df_filtered = df\n",
        "    df_filtered = df[df['labels'].apply(lambda x: any(x))]\n",
        "    \n",
        "    # Étape 5 : Exclure les lignes où les images sont corrompues\n",
        "    df_filtered = df_filtered[~df_filtered['image'].isin(bad_files)]\n",
        "\n",
        "    print(f'Dataframe filtered length: {len(df_filtered)}')\n",
        "\n",
        "    return df_filtered"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yeasR2LM88Pn",
        "outputId": "e9fb9243-0ccc-4900-853f-fe8a82012e14"
      },
      "outputs": [],
      "source": [
        "df = load_dataframe('./data/database.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "label_freq = df['labels'].apply(lambda x: x).explode().value_counts().sort_values(ascending=False)\n",
        "# Création de l'histogramme horizontal par ordre décroissant et plus grand\n",
        "plt.figure(figsize=(12, 20))\n",
        "label_freq.sort_values(ascending=True).plot(kind='barh')\n",
        "plt.xlabel('Frequency')\n",
        "plt.ylabel('Labels')\n",
        "plt.title('Frequency of Labels')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# # Create a list of rare labels\n",
        "rare = list(label_freq[label_freq<2000].index)\n",
        "\n",
        "# #rare = []\n",
        "\n",
        "# print(\"We will be ignoring these rare labels:\", rare)\n",
        "labels = load_labels()\n",
        "keep = [x for x in labels if x not in rare ]\n",
        "# print(keep)\n",
        "\n",
        "# print(len(rare))\n",
        "# print(len(keep))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Nettoyages des données et stratification\n",
        " Dataset :\n",
        " - train\n",
        " - val\n",
        " - test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['stratify_label'] = df['labels'].apply(lambda x: '|'.join(x))\n",
        "labels_count = df['stratify_label'].value_counts()\n",
        "valid_labels = labels_count[labels_count >= 20].index\n",
        "df_filtered = df[df['stratify_label'].isin(valid_labels)]\n",
        "print(len(df))\n",
        "print(len(df_filtered))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def custom_train_test_split(df, stratify_column, train_samples):\n",
        "    # Create initial train set of fixed number of samples\n",
        "    X_train, X_temp = train_test_split(\n",
        "        df, train_size=train_samples, stratify=df[stratify_column], random_state=42\n",
        "    )\n",
        "    \n",
        "    # Calculate the size of validation set (0.2% of train size)\n",
        "    val_size = int(0.1 * train_samples)\n",
        "    \n",
        "    # Create the validation set from the remaining data\n",
        "    X_val, X_temp,  = train_test_split(\n",
        "        X_temp, test_size=(len(X_temp) - val_size), stratify=X_temp[stratify_column], random_state=42\n",
        "    )\n",
        "    \n",
        "    # Calculate the size of test set (50% of validation size)\n",
        "    #test_size = int(0.5 * val_size)\n",
        "    test_size = val_size\n",
        "    \n",
        "    # Create the test set from the remaining data\n",
        "    X_test, X_unused = train_test_split(\n",
        "        X_temp, test_size=(len(X_temp) - test_size), stratify=X_temp[stratify_column], random_state=42\n",
        "    )\n",
        "    \n",
        "    return X_train, X_val, X_test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example usage:\n",
        "# X, y are your features and target DataFrame respectively\n",
        "# stratify_column is the name of the column in y used for stratification\n",
        "# train_samples is the fixed number of samples for the train set\n",
        "\n",
        "X_train, X_val, X_test = custom_train_test_split(df_filtered, 'stratify_label', 10000)\n",
        "\n",
        "print(f\"Train set size: {len(X_train)}\")\n",
        "print(f\"Validation set size: {len(X_val)}\")\n",
        "print(f\"Test set size: {len(X_test)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GShNzsCC88Po"
      },
      "outputs": [],
      "source": [
        "def write_df(df, labels, file_path):\n",
        "    rows = []\n",
        "    for index, row in df[['image', 'labels']].iterrows():\n",
        "        row_image = row['image']\n",
        "        row_labels = row['labels'].tolist()\n",
        "        row = {'image_name': row_image, 'image_labels': row_labels}\n",
        "        rows.append(row)\n",
        "    with open(file_path, 'w') as fp:\n",
        "        json.dump({'samples': rows, 'labels': labels}, fp, indent=3)\n",
        "\n",
        "write_df(X_train, keep, './data/train.json')\n",
        "write_df(X_val, keep, './data/val.json')\n",
        "write_df(X_test, keep, './data/test.json')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Création d'une classe pour le chargement des données"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhYm2YqdU0D-",
        "outputId": "9ad0ab0e-22b6-4c9b-9652-9b8fa63f9e3f"
      },
      "outputs": [],
      "source": [
        "class CustomDataGenerator(keras.utils.Sequence):\n",
        "    def __init__(self, file_path, batch_size=1, shuffle=True, **kwargs):\n",
        "        super().__init__(**kwargs)  # Appeler le constructeur de la classe parente\n",
        "        # self.imgs = image_filenames\n",
        "        # self.labels = labels\n",
        "        self.batch_size = batch_size\n",
        "        self.image_size = (224, 224)\n",
        "        #self.num_classes = num_classes\n",
        "        self.shuffle = shuffle\n",
        "\n",
        "\n",
        "        with open(file_path) as fp:\n",
        "            json_data = json.load(fp)\n",
        "        samples = json_data['samples']\n",
        "        self.classes = json_data['labels']\n",
        "        self.num_classes = len(self.classes)\n",
        "\n",
        "        self.imgs = []\n",
        "        self.vectors = []\n",
        "        self.labels = []\n",
        "        print('loading', file_path)\n",
        "        for sample in samples:\n",
        "            self.imgs.append(sample['image_name'])\n",
        "            labels = sample['image_labels']\n",
        "            self.labels.append(labels)\n",
        "\n",
        "            vector = [cls in labels for cls in self.classes]\n",
        "            self.vectors.append(np.array(vector, dtype=float))\n",
        "\n",
        "        self.on_epoch_end()\n",
        "        \n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.floor(len(self.imgs) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
        "        batch_filenames = [self.imgs[k] for k in indices]\n",
        "        batch_labels = [self.vectors[k] for k in indices]\n",
        "\n",
        "        X, y = self.__data_generation(batch_filenames, batch_labels)\n",
        "        return X, y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        self.indices = np.arange(len(self.imgs))\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indices)\n",
        "\n",
        "    def __data_generation(self, batch_filenames, batch_labels):\n",
        "        X = np.empty((self.batch_size, *self.image_size, 3), dtype=np.float32)\n",
        "        y = np.empty((self.batch_size, self.num_classes), dtype=int)\n",
        "\n",
        "        for i, (filename, label) in enumerate(zip(batch_filenames, batch_labels)):\n",
        "            image = Image.open(filename).resize(self.image_size)\n",
        "            image = np.array(image)\n",
        "            if image.shape == (self.image_size[0], self.image_size[1], 3):  # Check if image is RGB\n",
        "                #image = preprocess_input(image)\n",
        "                X[i,] = image\n",
        "                y[i] = label\n",
        "\n",
        "        return X, y\n",
        "    \n",
        "    def image_labels(self, idx):\n",
        "        #anno = self.annos[idx]\n",
        "        #img_labels = np.array(self.classes)[np.argwhere(anno > 0)[:, 0]]\n",
        "        img_labels = self.labels[idx]\n",
        "        img_path = self.imgs[idx]\n",
        "        image = Image.open(img_path)\n",
        "        return image, img_labels\n",
        "\n",
        "    def predictions(self, k, threshold, image, model):\n",
        "        def image_processing(image):\n",
        "            IMG_SIZE = 224\n",
        "            size = (IMG_SIZE, IMG_SIZE)    \n",
        "            image = tf.image.resize(image, size)\n",
        "            image = image.numpy()\n",
        "            return image    \n",
        "        # Assurez-vous que l'image est correctement chargée et traitée\n",
        "        test_img = image_processing(image)  # Correction: Utiliser image_path au lieu de image\n",
        "        test_img = np.expand_dims(test_img, axis=0)\n",
        "        \n",
        "        # Obtenir les prédictions brutes du modèle\n",
        "        raw_pred = model(test_img, training=False).cpu().numpy()[0]\n",
        "        \n",
        "        # Obtenir les indices des k meilleures prédictions\n",
        "        top_indices = np.argsort(raw_pred)[-k:][::-1]\n",
        "        \n",
        "        # Obtenir les étiquettes prédites pour les k meilleures prédictions\n",
        "        topk_labels = np.array(self.classes)[top_indices]\n",
        "        \n",
        "        # Afficher le nombre d'étiquettes prédites (devrait être k)\n",
        "        print(len(topk_labels))\n",
        "        \n",
        "        # Appliquer le seuil pour obtenir les indices des prédictions qui dépassent le seuil\n",
        "        threshold_indices = raw_pred > threshold\n",
        "        threshold_labels = np.array(self.classes)[np.where(threshold_indices)]\n",
        "        \n",
        "        # Afficher le nombre d'étiquettes qui dépassent le seuil\n",
        "        print(len(threshold_labels))\n",
        "\n",
        "\n",
        "        all_labels = set(topk_labels) | set(threshold_labels)\n",
        "\n",
        "        # Si aucune étiquette n'est prédite (très improbable avec les k meilleures prédictions), définir comme 'aucune prédiction'\n",
        "        if len(topk_labels) == 0:\n",
        "            topk_labels = ['No top k predictions']\n",
        "\n",
        "        # Si aucune étiquette ne dépasse le seuil, définir comme 'aucune prédiction'\n",
        "        if len(threshold_labels) == 0:\n",
        "            threshold_labels = ['No threshold predictions']\n",
        "            \n",
        "        return topk_labels, threshold_labels, all_labels\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exemples d'images avec labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XWjIfiaY88Po",
        "outputId": "77a79062-b9e0-4397-d7bf-9de5044ff0e1"
      },
      "outputs": [],
      "source": [
        "# Let's take a look at the data we have. To do it we need to load the dataset without augmentations.\n",
        "samples_ds = CustomDataGenerator('./data/test.json')\n",
        "\n",
        "for idx in range(5):\n",
        "    img, labels = samples_ds.image_labels(idx)\n",
        "    plt.imshow(img)\n",
        "    plt.title(\"{}\".format(', '.join(labels)))\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def show_histo(dataset,):\n",
        "    with np.printoptions(precision=3, suppress=True):\n",
        "        samples = dataset.vectors\n",
        "        samples = np.array(samples)\n",
        "        class_counts = np.sum(samples, axis=0)\n",
        "        # Sort labels according to their frequency in the dataset.\n",
        "        sorted_ids = np.array([i[0] for i in sorted(enumerate(class_counts), key=lambda x: x[1])], dtype=int)\n",
        "        print('Label distribution (count, class name):', list(zip(class_counts[sorted_ids].astype(int), np.array(dataset.classes)[sorted_ids])))\n",
        "        plt.figure(figsize=(12,20))\n",
        "        plt.barh(range(len(dataset.classes)), width=class_counts[sorted_ids])\n",
        "        plt.yticks(range(len(dataset.classes)), np.array(dataset.classes)[sorted_ids])\n",
        "        plt.gca().margins(y=0)\n",
        "        plt.grid()\n",
        "        plt.title('Label distribution')\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l9cp0DjG88Pp"
      },
      "outputs": [],
      "source": [
        "def build_model(num_classes):\n",
        "    inputs = layers.Input(shape=(224, 224, 3))\n",
        "\n",
        "    # On retire les dernières couches avec \"include_top=False\"\n",
        "    model = EfficientNetB0(include_top=False, input_tensor=inputs, weights=\"imagenet\")\n",
        "\n",
        "    # \"Glace\" the paramètres pré-entrainés\n",
        "    model.trainable = False\n",
        "\n",
        "    # On rajoute les couches retirées - ces couches sont entrainables !\n",
        "    x = layers.GlobalAveragePooling2D(name=\"avg_pool\")(model.output)\n",
        "\n",
        "    # On peut ajuster le ratio de neurones désactivés pour la couche Dropout\n",
        "    top_dropout_rate = 0.5\n",
        "    x = layers.Dropout(top_dropout_rate, name=\"top_dropout\")(x)\n",
        "    outputs = layers.Dense(num_classes, activation='sigmoid', name=\"predictions\")(x)\n",
        "\n",
        "    # Compile le nouveau modèle\n",
        "    model = keras.Model(inputs, outputs, name=\"EfficientNet\")\n",
        "\n",
        "    # On augmente le learning rate du défaut 0.001 à 0.01\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
        "\n",
        "    model.compile(\n",
        "         optimizer=optimizer,\n",
        "         loss=tf.keras.metrics.binary_crossentropy,\n",
        "         metrics=[\"binary_accuracy\"]\n",
        "    )\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePm3Zevd88Pq"
      },
      "outputs": [],
      "source": [
        "unfreeze = False\n",
        "def unfreeze_model(model):\n",
        "    # On rends entrainable les derniers 20 layers (sauf ceux de BatchNormalization)\n",
        "    for layer in model.layers[-20:]:\n",
        "        if not isinstance(layer, layers.BatchNormalization):\n",
        "          layer.trainable = True\n",
        "\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=1e-5)\n",
        "    model.compile(\n",
        "        optimizer=optimizer, loss=tf.keras.metrics.binary_crossentropy, metrics=[\"binary_accuracy\"]\n",
        "    )\n",
        "\n",
        "\n",
        "if unfreeze:\n",
        "    unfreeze_model(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# show_histo(train_generator)\n",
        "# show_histo(val_generator)\n",
        "# show_histo(test_generator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "train_generator = CustomDataGenerator('./data/train.json', batch_size=batch_size)\n",
        "val_generator = CustomDataGenerator('./data/val.json', batch_size=batch_size)\n",
        "test_generator = CustomDataGenerator('./data/test.json', batch_size=batch_size)\n",
        "\n",
        "model = build_model(train_generator.num_classes)\n",
        "history = model.fit(x=train_generator, validation_data=val_generator, epochs=10)\n",
        "model.save('simple_model_without_unfreeze.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Retrieve data from the history\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "binary_accuracy = history.history['binary_accuracy']\n",
        "val_binary_accuracy = history.history['val_binary_accuracy']\n",
        "\n",
        "# Plot the loss curves\n",
        "plt.figure(figsize=(14, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(loss, label='Training Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.title('Loss Curve')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Plot the accuracy curves\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(binary_accuracy, label='Training Accuracy')\n",
        "plt.plot(val_binary_accuracy, label='Validation Accuracy')\n",
        "plt.title('Accuracy Curve')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRQafyn588Pq"
      },
      "outputs": [],
      "source": [
        "model = keras.saving.load_model('simple_model_without_unfreeze.keras')\n",
        "model.evaluate(x=test_generator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_predictions(topk_predictions, treshold_predictions, image):\n",
        "    plt.imshow(image)\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "samples_ds = CustomDataGenerator('./data/test.json')\n",
        "model = keras.saving.load_model('simple_model_without_unfreeze.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "for idx in range(205, 215):\n",
        "    image, labels = samples_ds.image_labels(idx)\n",
        "    topk, treshold, all = samples_ds.predictions(3, 0.5, image, model)\n",
        "    print(topk)\n",
        "    print(treshold)\n",
        "    print(all)\n",
        "    print(labels)\n",
        "    plot_predictions([], [], image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "directory_path = './data/images_persos'\n",
        "\n",
        "# List all files and directories\n",
        "for entry in os.listdir(directory_path):\n",
        "    full_path = os.path.join(directory_path, entry)\n",
        "    if os.path.isfile(full_path):\n",
        "        print(entry)\n",
        "        image = Image.open(full_path)\n",
        "        topk, treshold, all = samples_ds.predictions(3, 0.5, image, model)\n",
        "        print(topk)\n",
        "        print(treshold)\n",
        "        print(all)\n",
        "        plot_predictions([], [], image)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
