{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ochekroun/projet-ia/blob/master/classification-images/Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15cc-9-f-5AQ"
      },
      "outputs": [],
      "source": [
        "# !python -V"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CR0SNc0n9Ytk"
      },
      "outputs": [],
      "source": [
        "#!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-Lt0QEL0E-5b",
        "outputId": "89066493-ac43-4206-c5b3-634c3e04f393"
      },
      "outputs": [],
      "source": [
        "# !pip install tensorflow==2.16.2\n",
        "# !pip install keras==3.4.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fV3J_EblNKDC"
      },
      "outputs": [],
      "source": [
        "# utilisons pytorch plutôt que tensorflow\n",
        "import os\n",
        "os.environ[\"KERAS_BACKEND\"] = \"torch\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "g_nFxcT388Pk"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-07-14 12:11:52.814005: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-07-14 12:11:52.841643: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "import keras\n",
        "from keras import layers\n",
        "from keras.applications import EfficientNetB0\n",
        "import requests\n",
        "import tarfile\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "import json\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPaAM2tDVPGG"
      },
      "outputs": [],
      "source": [
        "#!rm -rf ./data/nuswide_81/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKO1owWH88Pm",
        "outputId": "aa4c67b2-346f-4b94-f11b-7fd654f9d306"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 17%|█▋        | 839M/4.89G [00:26<02:08, 31.4MB/s] \n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[5], line 43\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m total_size \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m progress_bar\u001b[38;5;241m.\u001b[39mn \u001b[38;5;241m!=\u001b[39m total_size:\n\u001b[1;32m     40\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not download file\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 43\u001b[0m \u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttps://olivierchekroun.blob.core.windows.net/documents/projet-ia/nus_wide.tar.gz\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./data/test.tar.gz\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m decompress(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/test.tar.gz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/.test\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     46\u001b[0m download(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://olivierchekroun.blob.core.windows.net/documents/projet-ia/data.tar.gz\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/data.tar.gz\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "Cell \u001b[0;32mIn[5], line 35\u001b[0m, in \u001b[0;36mdownload\u001b[0;34m(url, file_path)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(total\u001b[38;5;241m=\u001b[39mtotal_size, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m, unit_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m progress_bar:\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m---> 35\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/projet-ia/lib/python3.12/site-packages/requests/models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
            "File \u001b[0;32m~/anaconda3/envs/projet-ia/lib/python3.12/site-packages/urllib3/response.py:1060\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1058\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1059\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1060\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1062\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[1;32m   1063\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
            "File \u001b[0;32m~/anaconda3/envs/projet-ia/lib/python3.12/site-packages/urllib3/response.py:949\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m amt:\n\u001b[1;32m    947\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer\u001b[38;5;241m.\u001b[39mget(amt)\n\u001b[0;32m--> 949\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    951\u001b[0m flush_decoder \u001b[38;5;241m=\u001b[39m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[1;32m    953\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
            "File \u001b[0;32m~/anaconda3/envs/projet-ia/lib/python3.12/site-packages/urllib3/response.py:873\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    870\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    872\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 873\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    874\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[1;32m    875\u001b[0m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[1;32m    876\u001b[0m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[1;32m    883\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mclose()\n",
            "File \u001b[0;32m~/anaconda3/envs/projet-ia/lib/python3.12/site-packages/urllib3/response.py:856\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1()\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
            "File \u001b[0;32m~/anaconda3/envs/projet-ia/lib/python3.12/http/client.py:479\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[1;32m    478\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[0;32m--> 479\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[1;32m    481\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
            "File \u001b[0;32m~/anaconda3/envs/projet-ia/lib/python3.12/socket.py:708\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 708\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    710\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/projet-ia/lib/python3.12/ssl.py:1252\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1250\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1251\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
            "File \u001b[0;32m~/anaconda3/envs/projet-ia/lib/python3.12/ssl.py:1104\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1105\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1106\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "download_file = True\n",
        "\n",
        "from pathlib import Path\n",
        "Path(\"./data\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "if download_file:\n",
        "    def decompress(tar_file, file_path, members=None):\n",
        "        \"\"\"\n",
        "        Extracts `tar_file` and puts the `members` to `path`.\n",
        "        If members is None, all members on `tar_file` will be extracted.\n",
        "        \"\"\"\n",
        "        tar = tarfile.open(tar_file, mode=\"r:gz\")\n",
        "        if members is None:\n",
        "            members = tar.getmembers()\n",
        "        # with progress bar\n",
        "        # set the progress bar\n",
        "        progress = tqdm(members)\n",
        "        for member in progress:\n",
        "            tar.extract(member, path=file_path)\n",
        "            # set the progress description of the progress bar\n",
        "            progress.set_description(f\"Extracting {member.name}\")\n",
        "\n",
        "        tar.close()\n",
        "\n",
        "    def download(url, file_path):\n",
        "        # Streaming, so we can iterate over the response.\n",
        "        response = requests.get(url, stream=True)\n",
        "\n",
        "        # Sizes in bytes.\n",
        "        total_size = int(response.headers.get(\"content-length\", 0))\n",
        "        block_size = 4096\n",
        "\n",
        "        with tqdm(total=total_size, unit=\"B\", unit_scale=True) as progress_bar:\n",
        "            with open(file_path, \"wb\") as file:\n",
        "                for data in response.iter_content(block_size):\n",
        "                    progress_bar.update(len(data))\n",
        "                    file.write(data)\n",
        "\n",
        "        if total_size != 0 and progress_bar.n != total_size:\n",
        "            raise RuntimeError(\"Could not download file\")\n",
        "\n",
        "\n",
        "    download('https://olivierchekroun.blob.core.windows.net/documents/projet-ia/nus_wide.tar.gz','./data/nus_wide.tar.gz')\n",
        "    decompress('./data/test.tar.gz', './data/nuswide_81')\n",
        "    \n",
        "    download('https://olivierchekroun.blob.core.windows.net/documents/projet-ia/data.tar.gz','./data/data.tar.gz')\n",
        "    decompress('./data/data.tar.gz', './data/')\n",
        "\n",
        "\n",
        "\n",
        "check_image_file = False\n",
        "if check_image_file:\n",
        "    image_dir = './data/images'\n",
        "    bad_image_file_path = './data/bad_images.txt'\n",
        "    IMG_SIZE = 224\n",
        "    size = (IMG_SIZE, IMG_SIZE)\n",
        "    count = 0\n",
        "    try:\n",
        "        os.remove(bad_image_file_path)\n",
        "    except OSError:\n",
        "        pass\n",
        "    arr = os.listdir(image_dir)\n",
        "    print(len(arr))\n",
        "    bad_files = []\n",
        "    for image_name in tqdm(arr):\n",
        "        count += 1\n",
        "        imagePath = os.path.join(image_dir, image_name)\n",
        "        try:\n",
        "            image = Image.open(imagePath)\n",
        "            image.load()\n",
        "            image = tf.image.resize(image, size)\n",
        "            image = image.numpy()\n",
        "        except Exception as e:\n",
        "            print(f'{e} {imagePath}')\n",
        "            bad_files.append(image_name)\n",
        "            continue\n",
        "    print(f'Images count: {count}')\n",
        "\n",
        "    print(bad_files)\n",
        "\n",
        "    with open(bad_image_file_path, 'w') as f:\n",
        "        for line in bad_files:\n",
        "            f.write(f\"{line}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9sdRxe2z88Pm"
      },
      "outputs": [],
      "source": [
        "def load_labels():\n",
        "    labels = []\n",
        "    with open('./data/labels.txt') as f:\n",
        "        lines = f.read().splitlines()\n",
        "        for line in lines:\n",
        "            labels.append(line)\n",
        "    return labels\n",
        "\n",
        "def load_dataframe(file_name):\n",
        "    with open('./data/bad_images.txt', 'r') as file:\n",
        "        bad_files = file.read().splitlines()\n",
        "\n",
        "    print(f'Bad files {bad_files}')\n",
        "\n",
        "    labels = load_labels()\n",
        "\n",
        "    # Étape 1 : Charger la liste des images\n",
        "\n",
        "    with open(file_name) as f:\n",
        "        all_samples_data = f.read().splitlines()\n",
        "\n",
        "\n",
        "    image_names =[]\n",
        "    string_values = []\n",
        "    array_values = []\n",
        "    given_labels = []\n",
        "    for data_line in tqdm(all_samples_data):\n",
        "        parts = data_line.split(' ')\n",
        "        image_name = parts[0]\n",
        "        array_value = np.asarray(list(map(int, parts[1:])))\n",
        "        string_value = ' '.join(parts[1:])\n",
        "\n",
        "        given_label = np.array(labels)[np.argwhere(array_value)[:, 0]]\n",
        "        #print(given_labels)\n",
        "\n",
        "        image_names.append(image_name)\n",
        "        string_values.append(string_value)\n",
        "        array_values.append(array_value)\n",
        "        given_labels.append(given_label)\n",
        "\n",
        "    # Étape 3 : Créer le DataFrame\n",
        "    data = {'Image': image_names, 'Values': array_values, 'String_Values': string_values, 'Given_Labels': given_labels}\n",
        "    df = pd.DataFrame(data)\n",
        "    print(f'Dataframe length: {len(df)}')\n",
        "\n",
        "    # Étape 4 : Exclure les lignes où il n'y a que des 0 dans les tableaux de valeurs\n",
        "    df_filtered = df\n",
        "    df_filtered = df[df['Values'].apply(lambda x: any(x))]\n",
        "    df_filtered = df_filtered[~df_filtered['Image'].isin(bad_files)]\n",
        "    # if stratify :\n",
        "    #     df_filtered = df_filtered.groupby('String_Values').filter(lambda x: len(x) > 1)\n",
        "    print(f'Dataframe filtered length: {len(df_filtered)}')\n",
        "\n",
        "    return df_filtered"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yeasR2LM88Pn",
        "outputId": "e9fb9243-0ccc-4900-853f-fe8a82012e14"
      },
      "outputs": [],
      "source": [
        "df = load_dataframe('./data/database.txt')\n",
        "labels = load_labels()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YnEqVKv888Pn",
        "outputId": "b44ac805-f933-4b48-f3a8-5fb72a8d63b7"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "stratify = False\n",
        "num_class = df['String_Values'].nunique()\n",
        "print(num_class)\n",
        "train_size = 1000\n",
        "val_size = num_class if stratify else int(train_size * 0.2)\n",
        "test_size = 0.5\n",
        "\n",
        "if stratify:\n",
        "    train_df, val_df = train_test_split(\n",
        "        df,\n",
        "        stratify = df['String_Values'].values,\n",
        "        train_size = train_size,\n",
        "        test_size= val_size\n",
        "        )\n",
        "else:\n",
        "    train_df, val_df = train_test_split(\n",
        "        df,\n",
        "        train_size = train_size,\n",
        "        test_size= val_size\n",
        "        )\n",
        "\n",
        "    train_df, test_df = train_test_split(\n",
        "        train_df,\n",
        "        test_size = test_size\n",
        "    )\n",
        "\n",
        "print(\"Number of images for training: \", len(train_df))\n",
        "print(\"Number of images for validation: \", len(val_df))\n",
        "print(\"Number of images for test: \", len(test_df))\n",
        "\n",
        "\n",
        "\n",
        "# df = load_dataframe('./data/nuswide_81/test.txt')\n",
        "# num_class = df['String_Values'].nunique()\n",
        "# print(num_class)\n",
        "# val_size = num_class if stratify else int(train_size * 0.2)\n",
        "# if stratify:\n",
        "#     test_df, _ = train_test_split(\n",
        "#         df,\n",
        "#         stratify = df['String_Values'].values,\n",
        "#         train_size=val_size\n",
        "#         )\n",
        "# else:\n",
        "#     test_df, _ = train_test_split(\n",
        "#     df,\n",
        "#     train_size=val_size,\n",
        "# )\n",
        "# print(\"Number of images for test: \", len(test_df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GShNzsCC88Po"
      },
      "outputs": [],
      "source": [
        "def write_df(df, labels, file_path):\n",
        "    rows = []\n",
        "    for index, row in test_df[['Image', 'Given_Labels']].iterrows():\n",
        "        row_image = row['Image']\n",
        "        row_labels = row['Given_Labels'].tolist()\n",
        "        row = {'image_name': row_image, 'image_labels': row_labels}\n",
        "        rows.append(row)\n",
        "    with open(file_path, 'w') as fp:\n",
        "        json.dump({'samples': rows, 'labels': labels}, fp, indent=3)\n",
        "\n",
        "write_df(train_df, labels, './data/train.json')\n",
        "write_df(val_df, labels, './data/val.json')\n",
        "write_df(test_df, labels, './data/test.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ZboKjdE88Po"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "class NusDataset(Dataset):\n",
        "    def __init__(self, anno_path, transforms):\n",
        "        self.transforms = transforms\n",
        "        with open(anno_path) as fp:\n",
        "            json_data = json.load(fp)\n",
        "        samples = json_data['samples']\n",
        "        self.classes = json_data['labels']\n",
        "\n",
        "        self.imgs = []\n",
        "        self.annos = []\n",
        "        print('loading', anno_path)\n",
        "        for sample in samples:\n",
        "            self.imgs.append(sample['image_name'])\n",
        "            self.annos.append(sample['image_labels'])\n",
        "        for item_id in range(len(self.annos)):\n",
        "            item = self.annos[item_id]\n",
        "            vector = [cls in item for cls in self.classes]\n",
        "            self.annos[item_id] = np.array(vector, dtype=float)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        anno = self.annos[item]\n",
        "        try:\n",
        "            IMG_SIZE = 224\n",
        "            size = (IMG_SIZE, IMG_SIZE)\n",
        "            img_path = self.imgs[item]\n",
        "            image = Image.open(img_path)\n",
        "            if self.transforms is not None:\n",
        "                image = self.transforms(image)\n",
        "            #image = tf.image.resize(image, size)\n",
        "            # image = image.numpy()\n",
        "        except Exception as e:\n",
        "            print(f'{e} {img_path}')\n",
        "        return image, anno\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n",
        "\n",
        "def image_processing(image):\n",
        "    IMG_SIZE = 224\n",
        "    size = (IMG_SIZE, IMG_SIZE)    \n",
        "    image = tf.image.resize(image, size)\n",
        "    image = image.numpy()\n",
        "    return image\n",
        "\n",
        "# Test preprocessing\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor()\n",
        "    #,    transforms.Normalize(mean, std)\n",
        "])\n",
        "#print(tuple(np.array(np.array(mean)*255).tolist()))\n",
        "\n",
        "# Train preprocessing\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(),\n",
        "    transforms.RandomAffine(degrees=20, translate=(0.2, 0.2), scale=(0.5, 1.5),                            shear=None),\n",
        "    transforms.ToTensor()\n",
        "    #,    transforms.Normalize(mean, std)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhYm2YqdU0D-",
        "outputId": "9ad0ab0e-22b6-4c9b-9652-9b8fa63f9e3f"
      },
      "outputs": [],
      "source": [
        "#!ls ./data/nuswide_81/images/11781_1590858672_d446580056_m.jpg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XWjIfiaY88Po",
        "outputId": "77a79062-b9e0-4397-d7bf-9de5044ff0e1"
      },
      "outputs": [],
      "source": [
        "# Let's take a look at the data we have. To do it we need to load the dataset without augmentations.\n",
        "samples_ds = NusDataset('./data/test.json', None)\n",
        "\n",
        "# A simple function for visualization.\n",
        "def show_sample(img, binary_img_labels):\n",
        "    # Convert the binary labels back to the text representation.\n",
        "    img_labels = np.array(samples_ds.classes)[np.argwhere(binary_img_labels > 0)[:, 0]]\n",
        "    plt.imshow(img)\n",
        "    plt.title(\"{}\".format(', '.join(img_labels)))\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "for sample_id in range(5):\n",
        "    img, labels = samples_ds[sample_id]\n",
        "    show_sample(img, labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jz9pwYxf88Po",
        "outputId": "978aaad1-ba79-457c-fb77-ebd4a4a4587b"
      },
      "outputs": [],
      "source": [
        "train_ds = NusDataset('./data/train.json', image_processing)\n",
        "val_ds = NusDataset('./data/val.json', image_processing)\n",
        "test_ds = NusDataset('./data/test.json', image_processing)\n",
        "\n",
        "train_dl = DataLoader(train_ds)\n",
        "val_dl = DataLoader(val_ds)\n",
        "test_dl = DataLoader(test_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l9cp0DjG88Pp"
      },
      "outputs": [],
      "source": [
        "def build_model(num_classes):\n",
        "    inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
        "\n",
        "    # On retire les dernières couches avec \"include_top=False\"\n",
        "    model = EfficientNetB0(include_top=False, input_tensor=inputs, weights=\"imagenet\")\n",
        "\n",
        "    # \"Glace\" the paramètres pré-entrainés\n",
        "    model.trainable = False\n",
        "\n",
        "    # On rajoute les couches retirées - ces couches sont entrainables !\n",
        "    x = layers.GlobalAveragePooling2D(name=\"avg_pool\")(model.output)\n",
        "\n",
        "    # On peut ajuster le ratio de neurones désactivés pour la couche Dropout\n",
        "    top_dropout_rate = 0.2\n",
        "    x = layers.Dropout(top_dropout_rate, name=\"top_dropout\")(x)\n",
        "    outputs = layers.Dense(num_classes, activation='sigmoid', name=\"predictions\")(x)\n",
        "\n",
        "    # Compile le nouveau modèle\n",
        "    model = keras.Model(inputs, outputs, name=\"EfficientNet\")\n",
        "\n",
        "    # On augmente le learning rate du défaut 0.001 à 0.01\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=1e-2)\n",
        "\n",
        "    model.compile(\n",
        "         optimizer=optimizer,\n",
        "         loss=tf.keras.metrics.binary_crossentropy,\n",
        "         metrics=[\"binary_accuracy\"]\n",
        "    )\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4hVOSA_88Pp"
      },
      "outputs": [],
      "source": [
        "NUM_CLASSES = 81\n",
        "IMG_SIZE = 224\n",
        "model = build_model(NUM_CLASSES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePm3Zevd88Pq"
      },
      "outputs": [],
      "source": [
        "def unfreeze_model(model):\n",
        "    # On rends entrainable les derniers 20 layers (sauf ceux de BatchNormalization)\n",
        "    for layer in model.layers[-20:]:\n",
        "        if not isinstance(layer, layers.BatchNormalization):\n",
        "          layer.trainable = True\n",
        "\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=1e-5)\n",
        "    model.compile(\n",
        "        optimizer=optimizer, loss=tf.keras.metrics.binary_crossentropy, metrics=[\"binary_accuracy\"]\n",
        "    )\n",
        "\n",
        "\n",
        "unfreeze_model(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qf_KC63xEVGu",
        "outputId": "0f751f5d-0307-4c67-fd45-c00a7d9f1c54"
      },
      "outputs": [],
      "source": [
        "#!pip list| grep keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPRNBK8Y88Pq",
        "outputId": "0464629f-eaa5-4d1c-b2c6-dd61d1749cff"
      },
      "outputs": [],
      "source": [
        "history = model.fit(x=train_dl,validation_data=val_dl,epochs=1)\n",
        "model.save('simple_model_without_unfreeze.keras')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRQafyn588Pq"
      },
      "outputs": [],
      "source": [
        "model = keras.saving.load_model('simple_model.keras')\n",
        "model.evaluate(x=test_dl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "l8Vo36lH88Pq",
        "outputId": "332ecc2a-db9f-45b2-986a-0606481a5dac"
      },
      "outputs": [],
      "source": [
        "# Run inference on the test data\n",
        "samples_ds = NusDataset('./data/test.json', image_processing)\n",
        "for sample_id in range(5, 15):\n",
        "    test_img, test_labels = samples_ds[sample_id]\n",
        "    test_img_path = samples_ds.imgs[sample_id]\n",
        "    test_img = np.expand_dims(test_img, axis=0)\n",
        "    # raw_pred  = model(test_img, training = False)[0]\n",
        "    raw_pred = model(test_img, training = False).cpu()[0]\n",
        "    raw_pred = np.array(raw_pred > 0.5, dtype=float)\n",
        "\n",
        "    predicted_labels = np.array(samples_ds.classes)[np.argwhere(raw_pred > 0)[:, 0]]\n",
        "    print(len(predicted_labels))\n",
        "    if not len(predicted_labels):\n",
        "        predicted_labels = ['no predictions']\n",
        "    img_labels = np.array(samples_ds.classes)[np.argwhere(test_labels > 0)[:, 0]]\n",
        "    plt.imshow(Image.open(test_img_path))\n",
        "    plt.title(\"Predicted labels: {} \\nGT labels: {}\".format(', '.join(predicted_labels), ', '.join(img_labels)))\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
